{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 10/31/23 last update by ED ###################\n",
    "#NOTE: \n",
    "    #Folders must be on the same folder layer as code\n",
    "    \n",
    "    #function_data = RePORTER local link for grant data on PMID 1999-2020\n",
    "    \n",
    "    #search_term_inputs >> BrandName_Linker.csv = your SearchID and Brand_Name cols (KEEP AS 2 COL, So Brand_Name col not always unique)\n",
    "    \n",
    "    #search_term_inputs >> SearchTerm_Drug and SearchTerm_TargetOnly.csv = Input / SearchID / TIME_CUT\n",
    "        #Input = PubMed Search term of intrest (DO NOT ADD \"NOT REVIEW[pt] or TIME RANGEs\", code adds them later)\n",
    "        #SearchID = This should be the Drug## or target number of the inputs (or whatever you want them to be, DO NOT LEAVE BLANK)\n",
    "            # for step 2 to work, Drug ID must start with 'drug' , and target ID must be all numbers (234 for example)\n",
    "            # if step 2 is not needed, any unique ID will work\n",
    "        #TIME_CUT = if you want to cut PMID download for each term at a set year (2021 = full capture ) 1980 > TIME_CUT\n",
    "            #Year of APY and PMID pub also part of final output, so this can be done later\n",
    "            #set at for year in range(1980, 2021) > auto add to search term at 5 download year intervals (bypass 10k per cycle cap) for full analysis\n",
    "\n",
    "\n",
    "#NOTE: csv / os / time / pandas should be a part of most python interface tools pre-load list\n",
    "#pip install biopython\n",
    "#pip install beautifulsoup4\n",
    "#pip install Bio\n",
    "\n",
    "\n",
    "from Bio import Entrez\n",
    "from Bio.Entrez import efetch\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv   \n",
    "import os\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step 1\n",
    "## CORE PMID DOWNLOAD STEPS (Have Search term inputs folder populated as seen in example format / ensure all other dependent folders are in place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PubYear CUT code set (use TIME_CUT col in search term CSV to set year for year stop like approval years)\n",
    "\n",
    "def pub_key_entz_target(Search_List):\n",
    "    Entrez.email = \"ezhou@bentley.edu\"  # USE YOUR EMAIL IF YOU'RE USING THIS CODE!!!!\n",
    "\n",
    "    # Initialize the output CSV with headers, if necessary\n",
    "    if not os.path.exists('output.csv'):\n",
    "        with open('output.csv', 'w') as f:\n",
    "            f.write(\"PMID,search_term,Search_ID,Modified_Search_Term,TIME_CUT\\n\")\n",
    "\n",
    "    for index, row in Search_List.iterrows():\n",
    "        search_term = row['search_term']\n",
    "        time_cut = int(row['TIME_CUT']) if not pd.isna(row['TIME_CUT']) else 2021\n",
    "        search_id = row['SearchID']\n",
    "\n",
    "        # Loop from 1980 to the time_cut value with a 1-year interval (bypass pubmed limited of 10k unique PMID per download request)\n",
    "        for year in range(1980, time_cut+1):\n",
    "            modified_search_term = f\"{search_term} AND ({year}:{year} [pdat])\"\n",
    "\n",
    "            handle = Entrez.esearch(db=\"pubmed\", term=modified_search_term, retmax=\"1500000\")\n",
    "            record = Entrez.read(handle)\n",
    "            entz = record[\"IdList\"]\n",
    "\n",
    "            search_output = pd.DataFrame(entz, columns=['PMID'])\n",
    "            search_output['search_term'] = search_term\n",
    "            search_output['Search_ID'] = search_id\n",
    "            search_output['Modified_Search_Term'] = modified_search_term    \n",
    "            search_output['TIME_CUT'] = '' if pd.isna(row['TIME_CUT']) else row['TIME_CUT']\n",
    "\n",
    "            search_output.to_csv('output.csv', mode='a', index=None, header=False)\n",
    "\n",
    "            time.sleep(0.9)  # Remove this if your input list is small. Too many requests/min can result in auto timeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchTerm_TargetOnly=pd.read_csv('search_term_inputs/SearchTerm_TargetOnly.csv').astype(str)\n",
    "SearchTerm_TargetOnly = SearchTerm_TargetOnly.rename(columns={'ID': 'SearchID'})\n",
    "SearchTerm_TargetOnly=SearchTerm_TargetOnly[['SearchID','TIME_CUT']]\n",
    "APP_YEAR_Target=SearchTerm_TargetOnly.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "SearchTerm_Drug=pd.read_csv('search_term_inputs/SearchTerm_Drug.csv').astype(str)\n",
    "SearchTerm_Drug = SearchTerm_Drug.rename(columns={'ID': 'SearchID'})\n",
    "SearchTerm_Drug=SearchTerm_Drug[['SearchID','TIME_CUT']]\n",
    "APP_YEAR_DRUG=SearchTerm_Drug.drop_duplicates()\n",
    "\n",
    "\n",
    "APP_YEAR_Search=APP_YEAR_Target.merge(APP_YEAR_DRUG,how='outer')\n",
    "APP_YEAR_Search=APP_YEAR_Search.dropna()\n",
    "APP_YEAR_Search.to_csv('new_data/Search_ID_Years.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchID</th>\n",
       "      <th>TIME_CUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>235</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>236</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>237</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drug394</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drug395</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>drug12</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>drug74</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drug165</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>drug55</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>drug170</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>drug88</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>drug396</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>drug397</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SearchID TIME_CUT\n",
       "0        53     2007\n",
       "1       235     2010\n",
       "2        63     2012\n",
       "3       135     2014\n",
       "4        48     2012\n",
       "5        98     2016\n",
       "6        33     2014\n",
       "7       236     1999\n",
       "8       237     2001\n",
       "9   drug394     2007\n",
       "10  drug395     2010\n",
       "11   drug12     2013\n",
       "12   drug74     2015\n",
       "13  drug165     2012\n",
       "14   drug55     2015\n",
       "15  drug170     2016\n",
       "16   drug88     2014\n",
       "17  drug396     1999\n",
       "18  drug397     2001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_YEAR_Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Target PMID Download +ID LINK +TIMECUT  #############################\n",
    "#DO NOT ADD \"AND (1998:2020 [pdat])\" to search term, \n",
    "inputkey='Target'\n",
    "\n",
    "mod=pd.read_csv('search_term_inputs/SearchTerm_TargetOnly.csv')\n",
    "\n",
    "mod['A']=\"(\"\n",
    "mod['C']=\")\"\n",
    "\n",
    "mod['search_term']=mod['A']+mod['Input']+mod['C']\n",
    "\n",
    "mod_fix=mod[['search_term','Input','TIME_CUT','SearchID']]\n",
    "mod_fix.to_csv('B_'+inputkey+'.csv', index=None)\n",
    "Term_list='B_'+inputkey+'.csv'\n",
    "\n",
    "target_list=pd.read_csv(Term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_key_entz_target(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headerList = ['PMID','search_term','SearchID','Modified_Search_Term','TIME_CUT']\n",
    "\n",
    "w0=pd.read_csv('output.csv')\n",
    "w0=w0.drop_duplicates()\n",
    "w0.to_csv('new_data/P_'+Term_list, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67710"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1=pd.read_csv('new_data/P_'+Term_list)\n",
    "\n",
    "ID_CORE=w1[['PMID','Search_ID']]\n",
    "ID_CORE=ID_CORE.drop_duplicates()\n",
    "ID_CORE = ID_CORE.rename(columns={'Search_ID': 'Target_ID'})\n",
    "ID_CORE.to_csv('new_data/ID_CORE_Target.csv', index=False)\n",
    "\n",
    "os.remove('output.csv')\n",
    "w1.to_csv('new_data/P_'+Term_list,index=False)\n",
    "w1['PMID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Drug PMID Download  #############################\n",
    "#DO NOT ADD \"AND (1998:2020 [pdat])\" to search term, \n",
    "inputkey='Drug'\n",
    "\n",
    "mod=pd.read_csv(('search_term_inputs/SearchTerm_Drug.csv'))\n",
    "#mod=pd.read_csv('Full_input_drug_only.csv')\n",
    "\n",
    "mod['A']=\"(\"\n",
    "mod['C']=\")\"\n",
    "\n",
    "mod['search_term']=mod['A']+mod['Input']+mod['C']\n",
    "\n",
    "mod_fix=mod[['search_term','Input','TIME_CUT','SearchID']]\n",
    "#mod_fix=mod[['search_term']]\n",
    "mod_fix.to_csv('B_'+inputkey+'.csv',index=None)\n",
    "Term_list='B_'+inputkey+'.csv'\n",
    "\n",
    "\n",
    "target_list=pd.read_csv(Term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_key_entz_target(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0=pd.read_csv('output.csv')\n",
    "w0=w0.drop_duplicates()\n",
    "w0.to_csv('new_data/P_'+Term_list, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1=pd.read_csv('new_data/P_'+Term_list)\n",
    "\n",
    "ID_CORE=w1[['PMID','Search_ID']]\n",
    "ID_CORE=ID_CORE.drop_duplicates()\n",
    "ID_CORE = ID_CORE.rename(columns={'Search_ID': 'Drug_ID'})\n",
    "ID_CORE.to_csv('new_data/ID_CORE_Drug.csv', index=False)\n",
    "\n",
    "\n",
    "w1.to_csv('new_data/P_'+Term_list,index=False)\n",
    "\n",
    "w1['PMID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_CORE_Total_Drug=pd.read_csv('new_data/ID_CORE_Target.csv')\n",
    "ID_CORE_Total_Target=pd.read_csv('new_data/ID_CORE_Drug.csv')\n",
    "\n",
    "ID_CORE_Total=ID_CORE_Total_Drug.merge(ID_CORE_Total_Target, how='outer')\n",
    "ID_CORE_Total=ID_CORE_Total.drop_duplicates()\n",
    "ID_CORE_Total=ID_CORE_Total.fillna(0)\n",
    "\n",
    "ID_CORE_Total.to_csv('new_data/PMID_Search_ID.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.98416676123937 min ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s min ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('B_Target.csv')\n",
    "os.remove('B_Drug.csv')\n",
    "\n",
    "os.remove('output.csv')\n",
    "\n",
    "os.remove('new_data/ID_CORE_Drug.csv')\n",
    "os.remove('new_data/ID_CORE_Target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step 2\n",
    "##################### Grant Funding Link Code (Start run here if working with local P_B_files from step 1)\n",
    "import csv   \n",
    "import os\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = pd.Timestamp(datetime.datetime(1994, 10, 10))\n",
    "TIME      = timestamp.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIH_Search_Drug(search_term_drug,search_term_target):\n",
    "\n",
    "###################### Ref Table Load ##################    \n",
    "    time_short = pd.read_csv('function_data/Reporter_pub_time_1980_2020_compact.csv',dtype=str)\n",
    "    core_set=pd.read_csv('function_data/Reporter_project_1985_2020_compact_sum.csv')\n",
    "    \n",
    "    core_set['TOTAL_COST'] = pd.to_numeric(core_set['TOTAL_COST'], errors='coerce').fillna(0)\n",
    "\n",
    "    \n",
    "    pub_key = pd.read_csv('function_data/Reporter_Link_Table_1980_2021_compact.csv',dtype=str)\n",
    "    \n",
    "    #Inflation_key_V2018 = pd.read_csv('function_data/inf2018_key.csv')  ### DO not use if Step 2 is also used (inflation change in Step 2 code too)\n",
    "    \n",
    "    Inflation_key_V2018 = pd.read_csv('function_data/inf_None_key.csv') #### code to not inflation adjust any years\n",
    "    \n",
    "    IC_Key = pd.read_csv('function_data/institute_key.csv',dtype=str)\n",
    "    \n",
    "###################### Inflation ##################\n",
    "    #core_set = core_set.loc[core_set['FY'] >=1985] 1985-2020 dataset loaded: 09/09/23\n",
    "    core_set_2018=pd.merge(core_set,Inflation_key_V2018, how='outer')\n",
    "    core_set_2018['inf_2018_costs']=core_set_2018['TOTAL_COST']*core_set_2018['inf_2018']\n",
    "    core_set_2018.drop('TOTAL_COST', inplace=True, axis=1)\n",
    "    core_set_2018.drop('inf_2018', inplace=True, axis=1)\n",
    "    core_set_2018.rename(columns = {\"inf_2018_costs\": \"TOTAL_COST\"}, \n",
    "              inplace = True)\n",
    "    core_set=core_set_2018\n",
    "    core_set_2018=1\n",
    "################################################# Drug vs Target PMID ############################### \n",
    "############### Drug #########\n",
    "    read_pmid = pd.read_csv('new_data/P_B_Drug.csv')\n",
    "    read_pmid_pmid_UQ=read_pmid[['PMID']]\n",
    "    read_pmid_pmid_UQ=read_pmid_pmid_UQ.drop_duplicates()\n",
    "    read_pmid_pmid_UQ['Data_Type']='Drug'\n",
    "    read_pmid_pmid_UQ.to_csv('new_data/pmid_drug.csv',index=None)\n",
    "    J_Drug_pmid_UQ = read_pmid_pmid_UQ\n",
    "    #os.remove(\"new_data/P_B_Drug.csv\")\n",
    "    \n",
    "###################### Target ##################    \n",
    "    read_pmid = pd.read_csv('new_data/P_B_Target.csv')\n",
    "    read_pmid_pmid_UQ=read_pmid[['PMID']]\n",
    "    read_pmid_pmid_UQ=read_pmid_pmid_UQ.drop_duplicates()\n",
    "    read_pmid_pmid_UQ['Data_Type']='Target'\n",
    "    read_pmid_pmid_UQ.to_csv('new_data/pmid_target.csv',index=None)\n",
    "    J_Target_pmid_UQ=read_pmid_pmid_UQ\n",
    "    #os.remove(\"new_data/P_B_Target.csv\")\n",
    "    \n",
    "##################### Drug \\ Target  ##########################################    \n",
    "    Drug_hold=pd.read_csv('new_data/pmid_drug.csv')\n",
    "    Target_hold=pd.read_csv('new_data/pmid_target.csv')\n",
    "    \n",
    "    Drug_hold_full=Drug_hold\n",
    "    Target_hold_full=Target_hold\n",
    "    Drug_hold=Drug_hold[['PMID']]\n",
    "    Target_hold=Target_hold[['PMID']]\n",
    "    Target_hold['search']='Target'\n",
    "    Drug_hold['search2']='Drug'\n",
    "    Drug_UQ_FULL=pd.merge(Drug_hold,Target_hold, how='outer')\n",
    "    Drug_UQ_FULL=Drug_UQ_FULL.fillna('only')\n",
    "    Drug_UQ_FULL['search_term']=Drug_UQ_FULL['search2']+Drug_UQ_FULL['search']\n",
    "    Drug_UQ_FULL_join=Drug_UQ_FULL[['PMID','search_term']]\n",
    "    Drug_UQ_FULL_join=Drug_UQ_FULL_join.drop_duplicates()\n",
    "    result = Drug_hold_full.append(Target_hold_full)\n",
    "    Type_Fix_PMID=pd.merge(result,Drug_UQ_FULL_join, how='outer')\n",
    "    Type_Fix_PMID=Type_Fix_PMID.drop_duplicates()\n",
    "    Type_Fix_PMID['search_term'] = Type_Fix_PMID['search_term'].replace({'DrugTarget':'Drug', 'Drugonly':'Drug'})\n",
    "    Type_Fix_PMID['search_term'] = Type_Fix_PMID['search_term'].replace({'onlyTarget':'TargetOnly', 'Drugonly':'Drug'})\n",
    "    Type_Fix_Drug=Type_Fix_PMID.loc[Type_Fix_PMID['search_term'] == 'Drug']\n",
    "    Type_Fix_TargetOnly=Type_Fix_PMID.loc[Type_Fix_PMID['search_term'] == 'TargetOnly']\n",
    "    \n",
    "###################### Drug NIH Funding ###########################################################################\n",
    "     \n",
    "    NIH_PMID=pd.merge(Type_Fix_Drug.astype(str),pub_key.astype(str), how='inner')\n",
    "    NIH_PMID=NIH_PMID.drop_duplicates()\n",
    "    NIH_PMID_time=pd.merge(NIH_PMID.astype(str),time_short.astype(str), how='inner')\n",
    "    NIH_PMID_time_core=pd.merge(core_set,NIH_PMID_time, how='inner')\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.fillna(0)\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.drop_duplicates()\n",
    "    \n",
    "###################### Pubyear_fixer    \n",
    "    Type_Fix_Drug_pubyear=NIH_PMID_time_core[['PMID','PUB_YEAR']]\n",
    "    Type_Fix_Drug_pubyear=Type_Fix_Drug_pubyear.drop_duplicates()\n",
    "    PB_d = Type_Fix_Drug_pubyear.groupby('PMID')['PUB_YEAR'].max().reset_index()\n",
    "    PB_d.columns = ['PMID', 'PUB_YEAR']\n",
    "    NIH_PMID_time_core.drop('PUB_YEAR', inplace=True, axis=1)\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.drop_duplicates()\n",
    "    #PB_d.to_csv('Pub_Year_SET1.csv')\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.merge(PB_d, how='inner')\n",
    "    \n",
    "    nih_test_start=NIH_PMID_time_core\n",
    "    nih_test_Last=NIH_PMID_time_core\n",
    "    nih_test1=NIH_PMID_time_core\n",
    "    \n",
    "###################### NIH_Funding_APY ##################  \n",
    "    nih_test_start2 = nih_test_start[nih_test_start.groupby('PROJECT_NUMBER')['FY'].transform('min') == nih_test_start['FY']]\n",
    "    nih_test_start2 = nih_test_start2[['PROJECT_NUMBER','FY']]\n",
    "    nih_test_start2=nih_test_start2.drop_duplicates()\n",
    "    nih_test_start2.rename(columns = {\"FY\": \"FY_Start\"}, \n",
    "              inplace = True)\n",
    "    nih_test_Last2 = nih_test_Last[nih_test_Last.groupby('PROJECT_NUMBER')['FY'].transform('max') == nih_test_start['FY']]\n",
    "    nih_test_Last2 = nih_test_Last2[['PROJECT_NUMBER','FY']]\n",
    "    nih_test_Last2=nih_test_Last2.drop_duplicates()\n",
    "    nih_test_Last2.rename(columns = {\"FY\": \"FY_Last\"}, \n",
    "              inplace = True)\n",
    "\n",
    "    nih_target_home = pd.merge(nih_test1, nih_test_start2,  how='left', left_on=['PROJECT_NUMBER'], right_on = ['PROJECT_NUMBER'])\n",
    "    nih_target_home=nih_target_home.drop_duplicates()\n",
    "    nih_target_home = pd.merge(nih_target_home, nih_test_Last2,  how='left', left_on=['PROJECT_NUMBER'], right_on = ['PROJECT_NUMBER'])\n",
    "    nih_target_home=nih_target_home.drop_duplicates()\n",
    "    \n",
    "    nih_target_home_sl=nih_target_home[['PROJECT_NUMBER','FY_Start','FY_Last','PMID','PUB_YEAR','TOTAL_COST','FY']]\n",
    "    nih_target_home_sl['FY_Last'] = nih_target_home_sl['FY_Last'].astype(int)\n",
    "    nih_target_home_sl['FY_Start'] = nih_target_home_sl['FY_Start'].astype(int)\n",
    "    nih_target_home_sl['PUB_YEAR'] = nih_target_home_sl['PUB_YEAR'].astype(int)\n",
    "    nih_target_home_sl['Pub_V_Start']=nih_target_home_sl['PUB_YEAR'] - nih_target_home_sl['FY_Start']\n",
    "    nih_target_home_sl['Pub_V_Last']=nih_target_home_sl['PUB_YEAR'] - nih_target_home_sl['FY_Last']\n",
    "    nih_target_home_sl = nih_target_home_sl.loc[nih_target_home_sl['Pub_V_Last'] <=4]\n",
    "    nih_target_home_sl = nih_target_home_sl.loc[nih_target_home_sl['Pub_V_Start'] >=0]\n",
    "    nih_target_home_sl.drop('Pub_V_Last', inplace=True, axis=1)\n",
    "    nih_target_home_sl.drop('Pub_V_Start', inplace=True, axis=1)\n",
    "    \n",
    "    APY_Cost_index=nih_target_home_sl[['PROJECT_NUMBER','TOTAL_COST','FY']]\n",
    "    APY_Cost_index=APY_Cost_index.drop_duplicates()\n",
    "    APY_Cost_index['TOTAL_COST'] = APY_Cost_index['TOTAL_COST'].astype(int)\n",
    "    g = APY_Cost_index.groupby(['PROJECT_NUMBER','FY'])['TOTAL_COST'].sum()\n",
    "    j = APY_Cost_index.groupby(['PROJECT_NUMBER','FY']).size().to_frame('count')\n",
    "    NIH_base=pd.merge(g, j, left_index=True, right_index=True).reset_index()\n",
    "    NIH_base.rename(columns = {\"FY\": \"APY\"}, \n",
    "              inplace = True)\n",
    "    \n",
    "    nih_test2=nih_target_home_sl\n",
    "    nih_test2['PUB_YEAR'] = nih_test2['PUB_YEAR'].astype(int)\n",
    "    nih_test2['FY_Last'] = nih_test2['FY_Last'].astype(int)\n",
    "    nih_test2['APY'] = nih_test2[['PUB_YEAR','FY_Last']].min(axis=1)\n",
    "    nih_test2['APY'] = nih_test2['APY'].astype(str)\n",
    "    nih_test2['PROJECT_NUMBER'] = nih_test2['PROJECT_NUMBER'].astype(str)\n",
    "    nih_test2[\"ACTUAL_PROJECT_YEAR\"] = nih_test2[\"APY\"] + nih_test2[\"PROJECT_NUMBER\"]\n",
    "    nih_test2.drop('TOTAL_COST', inplace=True, axis=1)\n",
    "    nih_test2.drop('FY', inplace=True, axis=1)\n",
    "\n",
    "    NIH_base['APY'] = NIH_base['APY'].astype(str)\n",
    "    nih_test2=pd.merge(nih_test2,NIH_base, how='inner')\n",
    "    nih_test2=nih_test2.drop_duplicates()  \n",
    "    \n",
    "    nih_test2.rename(columns = {\"count\": \"Project_Count\"}, \n",
    "          inplace = True)\n",
    "    nih_test2.rename(columns = {\"TOTAL_COST\": \"APY_COST_inf2018\"}, \n",
    "              inplace = True)\n",
    "    nih_test2=nih_test2.assign(Activity_Code=nih_test2['PROJECT_NUMBER'].str[:3])\n",
    "    nih_test2=nih_test2.assign(Institute_Code=nih_test2['PROJECT_NUMBER'].str[3:5])\n",
    "    nih_test2=pd.merge(nih_test2,IC_Key, how='inner')\n",
    "    nih_test2=nih_test2.drop_duplicates()\n",
    "    nih_test2['Search__ID']=search_term_drug\n",
    "    nih_test2['Search_Type']='Drug'\n",
    "\n",
    "    nih_test2=nih_test2[['Search__ID','Search_Type','PMID','PUB_YEAR','PROJECT_NUMBER','FY_Start','FY_Last','APY','ACTUAL_PROJECT_YEAR','APY_COST_inf2018','Activity_Code','Institute_Code','Acronym_institute_name','full_institute_name','Compressed Names','Project_Count']]\n",
    "    nih_test2.to_csv('new_data/Drug_hold.csv',index=None)\n",
    "    J_Drug_hold=nih_test2\n",
    "    \n",
    "    \n",
    "###################### Target NIH Funding ########################################################################### \n",
    "     \n",
    "    NIH_PMID=pd.merge(Type_Fix_TargetOnly.astype(str),pub_key.astype(str), how='inner')\n",
    "    NIH_PMID=NIH_PMID.drop_duplicates()\n",
    "    NIH_PMID_time=pd.merge(NIH_PMID.astype(str),time_short.astype(str), how='inner')\n",
    "    NIH_PMID_time_core=pd.merge(core_set,NIH_PMID_time, how='inner')\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.fillna(0)\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.drop_duplicates()\n",
    "    \n",
    "###################### Pubyear_fixer    \n",
    "    Type_Fix_Drug_pubyear=NIH_PMID_time_core[['PMID','PUB_YEAR']]\n",
    "    Type_Fix_Drug_pubyear=Type_Fix_Drug_pubyear.drop_duplicates()\n",
    "    PB_d = Type_Fix_Drug_pubyear.groupby('PMID')['PUB_YEAR'].max().reset_index()\n",
    "    PB_d.columns = ['PMID', 'PUB_YEAR']\n",
    "    NIH_PMID_time_core.drop('PUB_YEAR', inplace=True, axis=1)\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.drop_duplicates()\n",
    "    #PB_d.to_csv('Pub_Year_SET2.csv')\n",
    "    NIH_PMID_time_core=NIH_PMID_time_core.merge(PB_d, how='inner')\n",
    "    \n",
    "    nih_test_start=NIH_PMID_time_core\n",
    "    nih_test_Last=NIH_PMID_time_core\n",
    "    nih_test1=NIH_PMID_time_core\n",
    "    \n",
    "###################### NIH_Funding_APY ##################  \n",
    "    nih_test_start2 = nih_test_start[nih_test_start.groupby('PROJECT_NUMBER')['FY'].transform('min') == nih_test_start['FY']]\n",
    "    nih_test_start2 = nih_test_start2[['PROJECT_NUMBER','FY']]\n",
    "    nih_test_start2=nih_test_start2.drop_duplicates()\n",
    "    nih_test_start2.rename(columns = {\"FY\": \"FY_Start\"}, \n",
    "              inplace = True)\n",
    "    nih_test_Last2 = nih_test_Last[nih_test_Last.groupby('PROJECT_NUMBER')['FY'].transform('max') == nih_test_start['FY']]\n",
    "    nih_test_Last2 = nih_test_Last2[['PROJECT_NUMBER','FY']]\n",
    "    nih_test_Last2=nih_test_Last2.drop_duplicates()\n",
    "    nih_test_Last2.rename(columns = {\"FY\": \"FY_Last\"}, \n",
    "              inplace = True)\n",
    "\n",
    "    nih_target_home = pd.merge(nih_test1, nih_test_start2,  how='left', left_on=['PROJECT_NUMBER'], right_on = ['PROJECT_NUMBER'])\n",
    "    nih_target_home=nih_target_home.drop_duplicates()\n",
    "    nih_target_home = pd.merge(nih_target_home, nih_test_Last2,  how='left', left_on=['PROJECT_NUMBER'], right_on = ['PROJECT_NUMBER'])\n",
    "    nih_target_home=nih_target_home.drop_duplicates()\n",
    "    \n",
    "    nih_target_home_sl=nih_target_home[['PROJECT_NUMBER','FY_Start','FY_Last','PMID','PUB_YEAR','TOTAL_COST','FY']]\n",
    "    nih_target_home_sl['FY_Last'] = nih_target_home_sl['FY_Last'].astype(int)\n",
    "    nih_target_home_sl['FY_Start'] = nih_target_home_sl['FY_Start'].astype(int)\n",
    "    nih_target_home_sl['PUB_YEAR'] = nih_target_home_sl['PUB_YEAR'].astype(int)\n",
    "    nih_target_home_sl['Pub_V_Start']=nih_target_home_sl['PUB_YEAR'] - nih_target_home_sl['FY_Start']\n",
    "    nih_target_home_sl['Pub_V_Last']=nih_target_home_sl['PUB_YEAR'] - nih_target_home_sl['FY_Last']\n",
    "    nih_target_home_sl = nih_target_home_sl.loc[nih_target_home_sl['Pub_V_Last'] <=4]\n",
    "    nih_target_home_sl = nih_target_home_sl.loc[nih_target_home_sl['Pub_V_Start'] >=0]\n",
    "    nih_target_home_sl.drop('Pub_V_Last', inplace=True, axis=1)\n",
    "    nih_target_home_sl.drop('Pub_V_Start', inplace=True, axis=1)\n",
    "    \n",
    "    APY_Cost_index=nih_target_home_sl[['PROJECT_NUMBER','TOTAL_COST','FY']]\n",
    "    APY_Cost_index=APY_Cost_index.drop_duplicates()\n",
    "    APY_Cost_index['TOTAL_COST'] = APY_Cost_index['TOTAL_COST'].astype(int)\n",
    "    g = APY_Cost_index.groupby(['PROJECT_NUMBER','FY'])['TOTAL_COST'].sum()\n",
    "    j = APY_Cost_index.groupby(['PROJECT_NUMBER','FY']).size().to_frame('count')\n",
    "    NIH_base=pd.merge(g, j, left_index=True, right_index=True).reset_index()\n",
    "    NIH_base.rename(columns = {\"FY\": \"APY\"}, \n",
    "              inplace = True)\n",
    "    \n",
    "    nih_test2=nih_target_home_sl\n",
    "    nih_test2['PUB_YEAR'] = nih_test2['PUB_YEAR'].astype(int)\n",
    "    nih_test2['FY_Last'] = nih_test2['FY_Last'].astype(int)\n",
    "    nih_test2['APY'] = nih_test2[['PUB_YEAR','FY_Last']].min(axis=1)\n",
    "    nih_test2['APY'] = nih_test2['APY'].astype(str)\n",
    "    nih_test2['PROJECT_NUMBER'] = nih_test2['PROJECT_NUMBER'].astype(str)\n",
    "    nih_test2[\"ACTUAL_PROJECT_YEAR\"] = nih_test2[\"APY\"] + nih_test2[\"PROJECT_NUMBER\"]\n",
    "    nih_test2.drop('TOTAL_COST', inplace=True, axis=1)\n",
    "    nih_test2.drop('FY', inplace=True, axis=1)\n",
    "\n",
    "    NIH_base['APY'] = NIH_base['APY'].astype(str)\n",
    "    nih_test2=pd.merge(nih_test2,NIH_base, how='inner')\n",
    "    nih_test2=nih_test2.drop_duplicates()  \n",
    "    \n",
    "    nih_test2.rename(columns = {\"count\": \"Project_Count\"}, \n",
    "          inplace = True)\n",
    "    nih_test2.rename(columns = {\"TOTAL_COST\": \"APY_COST_inf2018\"}, \n",
    "              inplace = True)\n",
    "    nih_test2=nih_test2.assign(Activity_Code=nih_test2['PROJECT_NUMBER'].str[:3])\n",
    "    nih_test2=nih_test2.assign(Institute_Code=nih_test2['PROJECT_NUMBER'].str[3:5])\n",
    "    nih_test2=pd.merge(nih_test2,IC_Key, how='inner')\n",
    "    nih_test2=nih_test2.drop_duplicates()\n",
    "    nih_test2['Search__ID']=search_term_target\n",
    "    nih_test2['Search_Type']='target'\n",
    "\n",
    "    nih_test2=nih_test2[['Search__ID','Search_Type','PMID','PUB_YEAR','PROJECT_NUMBER','FY_Start','FY_Last','APY','ACTUAL_PROJECT_YEAR','APY_COST_inf2018','Activity_Code','Institute_Code','Acronym_institute_name','full_institute_name','Compressed Names','Project_Count']]\n",
    "    nih_test2.to_csv('new_data/Target_hold.csv',index=None)\n",
    "    J_Target_hold=nih_test2\n",
    "    \n",
    "###########################Drug vs Target Analysis########################################################\n",
    "\n",
    "#################### Search Terms ############################\n",
    "    Drug_hold=pd.read_csv('new_data/Drug_hold.csv')\n",
    "    Target_hold=pd.read_csv('new_data/Target_hold.csv')\n",
    "    \n",
    "    Drug_hold_full=Drug_hold\n",
    "    Target_hold_full=Target_hold\n",
    "    Drug_hold=Drug_hold[['ACTUAL_PROJECT_YEAR']]\n",
    "    Target_hold=Target_hold[['ACTUAL_PROJECT_YEAR']]\n",
    "    Target_hold['search']='Target'\n",
    "    Drug_hold['search2']='Drug'\n",
    "    Drug_UQ_FULL=pd.merge(Drug_hold,Target_hold, how='outer')\n",
    "    Drug_UQ_FULL=Drug_UQ_FULL.fillna('only')\n",
    "    Drug_UQ_FULL['search_term']=Drug_UQ_FULL['search2']+Drug_UQ_FULL['search']\n",
    "    Drug_UQ_FULL_join=Drug_UQ_FULL[['ACTUAL_PROJECT_YEAR','search_term']]\n",
    "    Drug_UQ_FULL_join=Drug_UQ_FULL_join.drop_duplicates()\n",
    "    result = Drug_hold_full.append(Target_hold_full)\n",
    "    resultUQ_FULL=pd.merge(result,Drug_UQ_FULL_join, how='outer')\n",
    "    resultUQ_FULL=resultUQ_FULL.drop_duplicates()\n",
    "    resultUQ_FULL['search_term'] = resultUQ_FULL['search_term'].replace({'DrugTarget':'Drug', 'Drugonly':'Drug'})\n",
    "    resultUQ_FULL['search_term'] = resultUQ_FULL['search_term'].replace({'onlyTarget':'TargetOnly', 'Drugonly':'Drug'})\n",
    "\n",
    "############# Project/Grant ID ##############################\n",
    "    Grant_code=pd.read_csv('function_data/Grant_Types.csv')\n",
    "    Grant_code=Grant_code.assign(Activity_Code=Grant_code['Activity_Code'].str[:3])\n",
    "    resultUQ_GT=pd.merge(resultUQ_FULL,Grant_code,how='outer')\n",
    "    resultUQ_GT = resultUQ_GT.dropna(axis=0, subset=['Search_Type'])\n",
    "    resultUQ_GT[\"Grant_Type_Name\"][resultUQ_GT['Activity_Code'].str.contains(\"Z\")] = \"Intramural Programs\"\n",
    "    resultUQ_GT=resultUQ_GT.fillna('Others')\n",
    "    resultUQ_GT=resultUQ_GT.drop_duplicates()\n",
    "    resultUQ_FULL=resultUQ_GT\n",
    "    \n",
    "    \n",
    "############# Cost per APY ##################################    \n",
    "    resultUQ_FULL=resultUQ_FULL[['Search__ID','Search_Type','search_term','PMID','PUB_YEAR','PROJECT_NUMBER','FY_Start','FY_Last','APY','ACTUAL_PROJECT_YEAR','APY_COST_inf2018','Activity_Code','Institute_Code','Acronym_institute_name','full_institute_name','Compressed Names','Project_Count','Grant_Type_Name']]\n",
    "    resultUQ_FULL.rename(columns = {\"Search_Type\": \"Source_Search_Type\"}, \n",
    "          inplace = True)\n",
    "    resultUQ_FULL.rename(columns = {\"search_term\": \"Search_Type\"}, \n",
    "          inplace = True)\n",
    "\n",
    "    resultUQ_FULL.to_csv('new_data/resultUQ_FULL.csv',index=None)\n",
    "    resultUQ_APY_COST=resultUQ_FULL[['APY','ACTUAL_PROJECT_YEAR','APY_COST_inf2018','Search_Type']]\n",
    "    resultUQ_APY_COST=resultUQ_APY_COST.drop_duplicates()\n",
    "    resultUQ_APY_COST.to_csv('new_data/resultUQ_APY_COST.csv',index=None)\n",
    "    J_resultUQ_APY_COST=resultUQ_APY_COST\n",
    "    J_resultUQ_FULL=resultUQ_FULL\n",
    "\n",
    "\n",
    "    \n",
    "########### Debug Cleanup  ####################    \n",
    "    core_set=1\n",
    "    time = 1\n",
    "    key = 1\n",
    "    NIH_PMID=1\n",
    "    time_short=1\n",
    "    J_resultUQ_FULL=1\n",
    "    J_resultUQ_APY_COST=1\n",
    "    resultUQ_APY_COST=1\n",
    "    resultUQ_FULL=1\n",
    "    Drug_UQ_FULL=1\n",
    "    Target_hold=1\n",
    "    resultUQ_GT=1\n",
    "    Grant_code=1\n",
    "    result=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIH_Search_Drug('APY_drug','APY_target_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.2875669956207275 min ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s min ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#0################################################################Input_PMID#')\n",
    "Grant_code=pd.read_csv('new_data/resultUQ_APY_COST.csv')\n",
    "Drug=Grant_code.loc[Grant_code['Search_Type'] == 'Drug']\n",
    "Target=Grant_code.loc[Grant_code['Search_Type'] == 'TargetOnly']\n",
    "Grant_code = Grant_code[['ACTUAL_PROJECT_YEAR','APY_COST_inf2018']]\n",
    "\n",
    "Grant_code_FULL=pd.read_csv('new_data/resultUQ_FULL.csv')\n",
    "Drug_PMID=Grant_code_FULL.loc[Grant_code_FULL['Search_Type'] == 'Drug']\n",
    "Drug_PMID=Drug_PMID[['PMID']]\n",
    "Target_PMID=Grant_code_FULL.loc[Grant_code_FULL['Search_Type'] == 'TargetOnly']\n",
    "Target_PMID=Target_PMID[['PMID']]\n",
    "Grant_code_FULL=Grant_code_FULL[['PMID']]\n",
    "\n",
    "Drug = Drug.drop_duplicates()\n",
    "Drug_PMID = Drug_PMID.drop_duplicates()\n",
    "Target_PMID = Target_PMID.drop_duplicates()\n",
    "Target = Target.drop_duplicates()\n",
    "Grant_code_FULL=Grant_code_FULL.drop_duplicates()\n",
    "\n",
    "PMID_DRUG=pd.read_csv('new_data/pmid_drug.csv')\n",
    "PMID_TARGET=pd.read_csv('new_data/pmid_target.csv')\n",
    "PMID_DRUG = PMID_DRUG.drop_duplicates()\n",
    "PMID_TARGET = PMID_TARGET.drop_duplicates()\n",
    "PMID_Full=pd.merge(PMID_DRUG,PMID_TARGET, how='outer')\n",
    "PMID_Full=PMID_Full.drop_duplicates()\n",
    "\n",
    "################  PMID  ###################################\n",
    "\n",
    "Drug_hold=pd.read_csv('new_data/pmid_drug.csv')\n",
    "Drug_hold=Drug_hold[['PMID']]\n",
    "Target_hold=pd.read_csv('new_data/pmid_target.csv')\n",
    "Target_hold=Target_hold[['PMID']]\n",
    "SET = pd.merge(Drug_hold, Target_hold, how='outer', indicator=True)\n",
    "SET=SET[['PMID','_merge']]\n",
    "SET = SET.drop_duplicates()\n",
    "\n",
    "Target1 = SET.loc[SET._merge == 'right_only', ['PMID']]\n",
    "Target1=Target1[['PMID']]\n",
    "Target1 = Target1.drop_duplicates()\n",
    "\n",
    "Drug1 = SET.loc[SET._merge == 'left_only', ['PMID']]\n",
    "Drug1=Drug1[['PMID']]\n",
    "Drug2 = SET.loc[SET._merge == 'both', ['PMID']]\n",
    "Drug2=Drug2[['PMID']]\n",
    "Drug_total = pd.merge(Drug1, Drug2, how='outer')\n",
    "Drug_total = Drug_total.drop_duplicates()\n",
    "\n",
    "Drug_Start_PMID_number=Drug_total.nunique()\n",
    "print(('Drug_PMID_number::::') + str(Drug_Start_PMID_number))\n",
    "\n",
    "Target_Start_PMID_number=Target1.nunique()\n",
    "print(('Target_PMID_number::::') + str(Target_Start_PMID_number))\n",
    "print('#1##############################################################Funded_PMID#')\n",
    "\n",
    "################ Funded PMID  ###################################\n",
    "\n",
    "Drug_hold=pd.read_csv('new_data/Drug_hold.csv')\n",
    "Drug_hold=Drug_hold[['PMID']]\n",
    "Target_hold=pd.read_csv('new_data/Target_hold.csv')\n",
    "Target_hold=Target_hold[['PMID']]\n",
    "SET = pd.merge(Drug_hold, Target_hold, how='outer', indicator=True)\n",
    "SET=SET[['PMID','_merge']]\n",
    "SET = SET.drop_duplicates()\n",
    "\n",
    "Target1 = SET.loc[SET._merge == 'right_only', ['PMID']]\n",
    "Target1=Target1[['PMID']]\n",
    "Target1 = Target1.drop_duplicates()\n",
    "\n",
    "Drug1 = SET.loc[SET._merge == 'left_only', ['PMID']]\n",
    "Drug1=Drug1[['PMID']]\n",
    "Drug2 = SET.loc[SET._merge == 'both', ['PMID']]\n",
    "Drug2=Drug2[['PMID']]\n",
    "Drug_total = pd.merge(Drug1, Drug2, how='outer')\n",
    "Drug_total = Drug_total.drop_duplicates()\n",
    "\n",
    "Drug_PMID_number=Drug_total.nunique()\n",
    "print(('Drug_PMID_number::::') + str(Drug_PMID_number))\n",
    "\n",
    "Target_PMID_number=Target1.nunique()\n",
    "print(('Target_PMID_number::::') + str(Target_PMID_number))\n",
    "print('#2#####################################################################APYs#')\n",
    "\n",
    "################ APY  ##################################################\n",
    "\n",
    "Target_APY=Target['ACTUAL_PROJECT_YEAR'].nunique()\n",
    "print(('Target_APY::::') + str(Target_APY))\n",
    "\n",
    "Drug_APY=Drug['ACTUAL_PROJECT_YEAR'].nunique()\n",
    "print(('Drug_APY::::') + str(Drug_APY))\n",
    "print('#3#########################################################NIH_Funding_Cost#')\n",
    "\n",
    "############# Cost of funding ##########################################################\n",
    "\n",
    "Target_cost=Target['APY_COST_inf2018'].sum(axis = 0, skipna = True)\n",
    "print(('Target_cost::::') + str(Target_cost))\n",
    "\n",
    "Drug_cost=Drug['APY_COST_inf2018'].sum(axis = 0, skipna = True)\n",
    "print(('Drug_cost::::') + str(Drug_cost))\n",
    "print('#4####################################################Total_Unique_analysis#')\n",
    "\n",
    "################## Total  ####################\n",
    "\n",
    "PMID_Full=PMID_Full[['PMID']]\n",
    "PMID_Full.astype(int)\n",
    "\n",
    "PMID_Full_number=PMID_Full.nunique()\n",
    "print(('Grant_code_ALL_PMID::::') + str(PMID_Full_number))\n",
    "\n",
    "Grant_code_FULL=Grant_code_FULL.nunique()\n",
    "print(('Grant_code_Funded_PMID::::') + str(Grant_code_FULL))\n",
    "\n",
    "Grant_code_APY=Grant_code['ACTUAL_PROJECT_YEAR'].nunique()\n",
    "print(('Grant_code_APY::::') + str(Grant_code_APY))\n",
    "\n",
    "Grant_code_cost=Grant_code['APY_COST_inf2018'].sum(axis = 0, skipna = True)\n",
    "print(('Grant_code_cost::::') + str(Grant_code_cost))\n",
    "\n",
    "Overview=pd.DataFrame(([['Drug_Input_PMID', int(Drug_Start_PMID_number)], \n",
    "                  ['Target_Input_PMID', int(Target_Start_PMID_number)], \n",
    "                  ['Drug_Funded_PMID', int(Drug_PMID_number)],\n",
    "                  ['Target_Funded_PMID', int(Target_PMID_number)], \n",
    "                  ['Target_APY', Target_APY],\n",
    "                  ['Drug_APY', Drug_APY], \n",
    "                  ['Target_cost', Target_cost],\n",
    "                  ['Drug_cost', Drug_cost], \n",
    "                  ['Unique_PMID', int(PMID_Full_number)],\n",
    "                  ['Unique_Funded_PMID', int(Grant_code_FULL)], \n",
    "                  ['Unique_APY', Grant_code_APY],\n",
    "                  ['Unique_Cost', Grant_code_cost],\n",
    "                  ['Analysis_Date', TIME]]), \n",
    "                columns=['Index', 'Amount'])\n",
    "Overview.to_csv('new_data/Overview.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
